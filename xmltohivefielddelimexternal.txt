import sys
import os
import findspark
findspark.init()
os.environ['JAVA_HOME']='/usr/lib/jvm/java'
os.environ['SPARK_HOME']='/usr/lib/spark'
from pyspark.sql import *
from pyspark.sql.functions import *
from pyspark.sql.types import *
import re

spark = SparkSession.builder.master("local[*]").appName("genre").enableHiveSupport().getOrCreate()
sc = spark.sparkContext
data="s3://rus2022/emp/complexxmldata.xml"
#op="s3://rus2022/emp/xmlopdata/"
df=spark.read.format("xml").option("rowTag","catalog_item").option("path",data).load()

def read_nested_json(df):
    column_list = []
    for column_name in df.schema.names:
        if isinstance(df.schema[column_name].dataType, ArrayType):
            df = df.withColumn(column_name, explode(column_name))
            column_list.append(column_name)
        elif isinstance(df.schema[column_name].dataType, StructType):
            for field in df.schema[column_name].dataType.fields:
                column_list.append(col(column_name + "." + field.name)\
                                   .alias(column_name + "_" + field.name))
        else:
            column_list.append(column_name)
    df = df.select(column_list)
    return df;

def flatten(df):
    read_nested_json_flag = True
    while read_nested_json_flag:
        df = read_nested_json(df);
        read_nested_json_flag = False
        for column_name in df.schema.names:
            if isinstance(df.schema[column_name].dataType, ArrayType):
                read_nested_json_flag = True
            elif isinstance(df.schema[column_name].dataType, StructType):
                read_nested_json_flag = True;
    cols = [re.sub('[^a-zA-Z0-1]', "", c.lower()) for c in df.columns]
    return df.toDF(*cols);

ndf=flatten(df)
spark.sql('set hive.exec.dynamic.partition.mode=nonstrict')
spark.sql('set hive.exec.dynamic.partition=true')
ndf.write.mode('overwrite').format('hive').partitionBy('sizecolorswatchvalue').option('path','s3://rus2022/emp/xmloppartn').option('field.delim',',').saveAsTable('xml2partn')
#ndf.toPandas().to_csv(op)
ndf.show()
#ndf.printSchema()
